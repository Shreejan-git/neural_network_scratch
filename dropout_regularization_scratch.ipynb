{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2a28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e7566",
   "metadata": {},
   "source": [
    "There are various types of dropouts. Below, I have done the scratch implementation of the inverted dropout. Inverted dropout has one simple twist in its implementation that makes it more optimized than the original standard dropout.\n",
    "\n",
    "In Inverted dropout, after eliminating the neurons, we divide all the remaining neurons by the probability of keeping them. For example: if our dropout rate is 40% then we will keep the remaining 60% of the neurons activated. That means we will divide all the remaining neurons by 0.6 (probabilistic value). \n",
    "\n",
    "We have to do this because, when we randomly remove the neurons by making them zero, it will also decrease the value of the mean (expected value) of the layers by the rate of the dropout. For example: if we had 10 neurons and we dropped 50% i.e., 5 neurons, then the possibility that its expected value will also get reduced by 50%. \n",
    "This will cause a problem in the testing time since we do not use dropout while testing the model. So, to make this balance, scaling on the training phase like the above-mentioned step is done in inverted dropout.\n",
    "There are other effective dropouts as well. Some of them are gaussian dropout and monte Carlo dropout. \n",
    "\n",
    "The dropout which I have implemented is used in the dense layer (fully connected layer). I read many blogs and article and what I found is we should avoide using dropout in convulation layers in CNN and same with the RNN. Inverted dropout should only be used in dense layers. Instead, we should use batch normalization to get the better result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a445492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[2],[3]]) #Just a dummy data to test the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bfecd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9d1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbcc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e135ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_pass(Z,activation,keepprob):\n",
    "    #implementation of inverted dropout\n",
    "    if activation == 'relu':\n",
    "        a = np.where(0,Z<0,Z)\n",
    "    \n",
    "    else:\n",
    "        a = 1/(1+np.exp(-Z))\n",
    "        \n",
    "    mask = np.random.rand(a.shape[0],a.shape[1]) < keepprob\n",
    "    a = np.multiply(a,mask.astype(int))\n",
    "    return a/keepprob\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198d4ff",
   "metadata": {},
   "source": [
    "Dividing a by keepprob prevents networkâ€™s activations from getting too large, and does not require any changes to the network during evaluation. In contrast, traditional dropout requires scaling to be implemented during the test phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0c8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_initializer(l):\n",
    "    '''\n",
    "    initializes the parameters for each layer. \n",
    "    '''\n",
    "    parameters = {}\n",
    "    for i in range(1,len(l)):\n",
    "        parameters[\"W\"+str(i)] = np.random.randn(l[i],l[i-1])\n",
    "        parameters[\"b\"+str(i)] = np.zeros((l[i],1))\n",
    "        \n",
    "    print(parameters,'\\n')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae8ba94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(X,W,b):\n",
    "    #W.X + b\n",
    "    z = np.dot(W,X) + b  \n",
    "    return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a16850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(Y,yhat):\n",
    "    '''\n",
    "    binary cross-entropy as a cost function.\n",
    "    '''\n",
    "    m = Y.shape[1]\n",
    "    return np.multiply(-1/m,np.dot(Y,yhat) + np.dot((1-Y),(1-yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45bd7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,L,Y,keepprob=1):\n",
    "    '''\n",
    "    This is not full-fledged forward propagation. I will implement full-fledged soon.\n",
    "    '''\n",
    "    layer = len(L)\n",
    "    parameters = parameters_initializer(L)\n",
    "\n",
    "    X = X\n",
    "    caches = {}\n",
    "    \n",
    "    \n",
    "    for i in range(1,layer-1): #for hidden layers\n",
    "        z = linear_forward(X, parameters[\"W\"+str(i)], parameters[\"b\"+str(i)])\n",
    "        a = activation_pass(z,'relu',keepprob)\n",
    "        X = a\n",
    "        caches['z' + str(i)] = z\n",
    "        caches[\"a\" + str(i)] = a\n",
    "        \n",
    "    z = linear_forward(X, parameters[\"W\"+str(layer-1)], parameters['b'+str(layer-1)]) #for output layer\n",
    "    a = activation_pass(z,'sigmoid',keepprob)\n",
    "    X = a\n",
    "    caches['z' + str(i)] = z\n",
    "    caches[\"a\" + str(i)] = a\n",
    "    \n",
    "    cost_val = cost_function(Y,X)\n",
    "    \n",
    "    return caches, cost_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f1c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-0.71314354,  0.11057287],\n",
      "       [ 0.96394978, -0.37549555],\n",
      "       [ 0.34669511,  0.30641827],\n",
      "       [-1.01993101, -2.54204545]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 0.98671014, -0.79219814,  0.86651908,  0.51284834],\n",
      "       [ 1.16395767,  0.27965488, -1.5279927 , -0.25330069],\n",
      "       [ 1.03145572,  1.55036054,  0.6327283 , -2.10131863],\n",
      "       [ 1.15807224, -1.13090936,  1.16413935,  0.69559121]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[-1.8885206 , -1.70873625, -0.75854098,  0.49487076],\n",
      "       [-0.18948547,  0.82294103, -1.1826424 , -0.3775646 ],\n",
      "       [ 1.03470308,  0.39170679, -0.73598235, -1.2953592 ],\n",
      "       [ 1.17522386,  0.96359291, -0.0342983 , -1.54382562],\n",
      "       [-0.00510098,  2.87909031, -0.01801583, -1.72076018]]), 'b3': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W4': array([[ 1.83231561,  1.00296196, -1.31656502, -1.62101661, -1.38368064]]), 'b4': array([[0.]])} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'z1': array([[-1.09456847],\n",
       "         [ 0.80141292],\n",
       "         [ 1.61264501],\n",
       "         [-9.66599837]]),\n",
       "  'a1': array([[-1.09456847],\n",
       "         [ 0.80141292],\n",
       "         [ 1.61264501],\n",
       "         [-9.66599837]]),\n",
       "  'z2': array([[-5.27470319],\n",
       "         [-1.06561805],\n",
       "         [21.44518868],\n",
       "         [-7.02015471]]),\n",
       "  'a2': array([[-5.27470319],\n",
       "         [-1.06561805],\n",
       "         [21.44518868],\n",
       "         [-7.02015471]]),\n",
       "  'z3': array([[-37.33191721]]),\n",
       "  'a3': array([[6.12286038e-17]])},\n",
       " array([[-6.12286038e-17]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = [2,4,4,5,1] #number of layers and neurons in each layer.\n",
    "forward_prop(X,L,Y,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
